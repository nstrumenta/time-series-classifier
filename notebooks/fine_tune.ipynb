{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Model with Synthetic Data\n",
    "\n",
    "This notebook demonstrates training a magnetic distortion classifier using synthetically generated sensor data. This approach eliminates the need for external sensor logs and provides full control over the training data characteristics.\n",
    "\n",
    "## Process:\n",
    "1. **Generate Training Data**: Create multiple synthetic datasets with different magnetic distortion levels\n",
    "2. **Create Spectrograms**: Convert time-series data to spectrograms for audio classification\n",
    "3. **Prepare Dataset**: Build HuggingFace dataset with proper labels and splitting\n",
    "4. **Configure Model**: Set up Audio Spectrogram Transformer (AST) for magnetic distortion classification\n",
    "5. **Train Model**: Fine-tune the model with configurable parameters\n",
    "6. **Evaluate**: Test the model and save for deployment\n",
    "\n",
    "## Key Features:\n",
    "- **Self-contained**: Generates its own training data - no external dependencies\n",
    "- **Configurable**: Easy to adjust distortion levels, data amounts, and training parameters\n",
    "- **Debug Mode**: Fast training for testing (`DEBUG_MODE = True`)\n",
    "- **Production Ready**: Full training capabilities for deployment\n",
    "- **Modular**: Uses new modular architecture for maintainability\n",
    "\n",
    "## Configuration:\n",
    "- Set `DEBUG_MODE = True` for fast testing with minimal data\n",
    "- Adjust `NUM_SEQUENCES` to control amount of training data\n",
    "- Modify distortion parameters to customize the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install datasets[audio]==3.0.1\n",
    "%pip install mcap==1.2.1\n",
    "%pip install torch\n",
    "%pip install torchaudio\n",
    "%pip install transformers[torch]==4.46.2\n",
    "%pip install nstrumenta==0.1.3\n",
    "%pip install evaluate\n",
    "%pip install numpy\n",
    "\n",
    "# Clone repository if in Colab (needed for source files and utilities)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"üîÑ Detected Google Colab - setting up repository...\")\n",
    "    \n",
    "    # Check if repo is already cloned\n",
    "    if not os.path.exists(\"time-series-classifier\"):\n",
    "        print(\"üì• Cloning time-series-classifier repository...\")\n",
    "        !git clone https://github.com/nstrumenta/time-series-classifier.git\n",
    "    else:\n",
    "        print(\"‚úì Repository already exists\")\n",
    "    \n",
    "    # Change to repo directory\n",
    "    %cd time-series-classifier\n",
    "    print(f\"‚úì Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"üñ•Ô∏è Detected local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup paths for both Colab and local environments\n",
    "def setup_environment():\n",
    "    \"\"\"Setup environment for both Colab and local development\"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Determine if we're in Colab and adjust paths accordingly\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        # In Colab, we should be in the repo directory after the previous cell\n",
    "        src_dir = os.path.join(current_dir, \"src\")\n",
    "        scripts_dir = os.path.join(current_dir, \"scripts\")\n",
    "    else:\n",
    "        # Local development - find repo root\n",
    "        # Look for src directory in current or parent directories\n",
    "        check_dirs = [current_dir, os.path.dirname(current_dir), os.path.join(current_dir, \"..\")]\n",
    "        src_dir = None\n",
    "        scripts_dir = None\n",
    "        \n",
    "        for check_dir in check_dirs:\n",
    "            potential_src = os.path.join(check_dir, \"src\")\n",
    "            potential_scripts = os.path.join(check_dir, \"scripts\")\n",
    "            if os.path.exists(potential_src) and os.path.exists(potential_scripts):\n",
    "                src_dir = potential_src\n",
    "                scripts_dir = potential_scripts\n",
    "                break\n",
    "        \n",
    "        if not src_dir:\n",
    "            # Fallback to current directory structure\n",
    "            src_dir = os.path.abspath(os.path.join(current_dir, \"src\"))\n",
    "            scripts_dir = os.path.abspath(os.path.join(current_dir, \"scripts\"))\n",
    "    \n",
    "    # Add paths to sys.path\n",
    "    for path in [src_dir, scripts_dir]:\n",
    "        if os.path.exists(path) and path not in sys.path:\n",
    "            sys.path.append(path)\n",
    "            print(f\"‚úì Added to path: {path}\")\n",
    "    \n",
    "    return src_dir, scripts_dir\n",
    "\n",
    "# Setup environment\n",
    "src_dir, scripts_dir = setup_environment()\n",
    "\n",
    "# Import utilities with fallback handling\n",
    "def import_with_fallback():\n",
    "    \"\"\"Import modules with graceful fallbacks\"\"\"\n",
    "    nst_client = None\n",
    "    imports = {}\n",
    "    \n",
    "    try:\n",
    "        # Try new modular imports first\n",
    "        from script_utils import init_script_environment\n",
    "        src_dir, nst_client = init_script_environment()\n",
    "        imports['script_utils'] = True\n",
    "        print(\"‚úì Using new script_utils environment setup\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è script_utils not available, using manual setup...\")\n",
    "        imports['script_utils'] = False\n",
    "        \n",
    "        # Manual Nstrumenta client setup\n",
    "        from nstrumenta import NstrumentaClient\n",
    "        \n",
    "        if \"google.colab\" in sys.modules:\n",
    "            from google.colab import userdata\n",
    "            os.environ[\"NSTRUMENTA_API_KEY\"] = userdata.get(\"NSTRUMENTA_API_KEY\")\n",
    "        \n",
    "        nst_client = NstrumentaClient(os.getenv(\"NSTRUMENTA_API_KEY\"))\n",
    "        \n",
    "        try:\n",
    "            print(f\"‚úì Connected to project: {nst_client.get_project()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not verify project connection: {e}\")\n",
    "    \n",
    "    # Try importing project modules\n",
    "    try:\n",
    "        from mcap_utils import create_dataset, spectrogram_from_timeseries\n",
    "        imports['mcap_utils'] = 'modular'\n",
    "        print(\"‚úì Using new modular mcap_utils\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            import mcap_utilities\n",
    "            imports['mcap_utils'] = 'legacy'\n",
    "            create_dataset = mcap_utilities.create_dataset\n",
    "            spectrogram_from_timeseries = mcap_utilities.spectrogram_from_timeseries\n",
    "            print(\"‚ö†Ô∏è Using legacy mcap_utilities\")\n",
    "        except ImportError:\n",
    "            imports['mcap_utils'] = 'failed'\n",
    "            create_dataset = None\n",
    "            spectrogram_from_timeseries = None\n",
    "            print(\"‚ùå Could not import MCAP utilities\")\n",
    "    \n",
    "    try:\n",
    "        from synthetic import SyntheticDataGenerator\n",
    "        imports['synthetic'] = True\n",
    "        print(\"‚úì Synthetic data generator available\")\n",
    "    except ImportError:\n",
    "        imports['synthetic'] = False\n",
    "        SyntheticDataGenerator = None\n",
    "        print(\"‚ö†Ô∏è Synthetic data generator not available\")\n",
    "    \n",
    "    return nst_client, imports, create_dataset, spectrogram_from_timeseries, SyntheticDataGenerator\n",
    "\n",
    "# Initialize environment\n",
    "nst_client, imports, create_dataset, spectrogram_from_timeseries, SyntheticDataGenerator = import_with_fallback()\n",
    "\n",
    "print(f\"\\nüìã Environment Summary:\")\n",
    "print(f\"  - Script utils: {'‚úì' if imports.get('script_utils') else '‚ö†Ô∏è'}\")\n",
    "print(f\"  - MCAP utils: {imports.get('mcap_utils', 'failed')}\")\n",
    "print(f\"  - Synthetic: {'‚úì' if imports.get('synthetic') else '‚ö†Ô∏è'}\")\n",
    "print(f\"  - Working directory: {os.getcwd()}\")\n",
    "print(f\"  - Python path includes: {len([p for p in sys.path if 'src' in p or 'scripts' in p])} project directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for synthetic data generation and training\n",
    "import uuid\n",
    "\n",
    "# Training mode configuration\n",
    "DEBUG_MODE = True  # Set to False for full training\n",
    "\n",
    "# Synthetic data generation configuration\n",
    "NUM_SEQUENCES = 2 if DEBUG_MODE else 8  # Number of synthetic sequences to generate\n",
    "SEQUENCE_DURATION = 60.0  # Duration of each sequence in seconds\n",
    "SAMPLE_RATE = 100  # Sample rate for synthetic data\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    model_id = \"MAG_DIST_DEBUG\"\n",
    "    max_steps = 10\n",
    "    num_train_epochs = 1\n",
    "    per_device_train_batch_size = 1\n",
    "    per_device_eval_batch_size = 1\n",
    "    eval_steps = 5\n",
    "    print(\"üêõ DEBUG MODE ENABLED\")\n",
    "    print(\"Using reduced parameters for fast testing\")\n",
    "else:\n",
    "    model_id = \"MAG_DIST_SYNTHETIC\"\n",
    "    max_steps = 300\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 2\n",
    "    per_device_eval_batch_size = 4\n",
    "    eval_steps = 50\n",
    "    print(\"üöÄ FULL TRAINING MODE\")\n",
    "    print(\"Using full parameters for production training\")\n",
    "\n",
    "# Generate unique training session ID\n",
    "session_id = str(uuid.uuid4())[:8]\n",
    "working_folder = f\"./temp/{model_id}_{session_id}\"\n",
    "\n",
    "print(f\"üéØ Training Session: {session_id}\")\n",
    "print(f\"üìä Will generate {NUM_SEQUENCES} synthetic sequences\")\n",
    "print(f\"‚è±Ô∏è  Each sequence: {SEQUENCE_DURATION} seconds\")\n",
    "print(f\"üìà Sample rate: {SAMPLE_RATE} Hz\")\n",
    "\n",
    "# Working directory management with cross-platform support\n",
    "def setup_working_directory_portable(path):\n",
    "    \"\"\"Setup working directory that works in both Colab and local environments\"\"\"\n",
    "    abs_path = os.path.abspath(path)\n",
    "    os.makedirs(abs_path, exist_ok=True)\n",
    "    os.chdir(abs_path)\n",
    "    print(f\"‚úì Working directory set to: {os.getcwd()}\")\n",
    "    return abs_path\n",
    "\n",
    "def reset_to_initial_directory():\n",
    "    \"\"\"Reset to initial directory with environment detection\"\"\"\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        # In Colab, go back to the repo root\n",
    "        repo_indicators = [\".git\", \"src\", \"scripts\", \"notebooks\"]\n",
    "        current = os.getcwd()\n",
    "        \n",
    "        # Navigate up until we find repo root\n",
    "        while current != \"/\" and not all(os.path.exists(os.path.join(current, indicator)) for indicator in repo_indicators[:2]):\n",
    "            current = os.path.dirname(current)\n",
    "        \n",
    "        if current != \"/\":\n",
    "            os.chdir(current)\n",
    "            print(f\"‚úì Reset to repo root: {os.getcwd()}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not find repo root, staying in current directory\")\n",
    "    else:\n",
    "        # Local environment\n",
    "        try:\n",
    "            if imports.get('script_utils'):\n",
    "                from script_utils import reset_to_initial_cwd\n",
    "                reset_to_initial_cwd()\n",
    "            else:\n",
    "                print(f\"Current working directory: {os.getcwd()}\")\n",
    "        except (ImportError, NameError):\n",
    "            print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Reset to appropriate starting directory\n",
    "reset_to_initial_directory()\n",
    "\n",
    "# Setup working directory\n",
    "try:\n",
    "    if imports.get('script_utils'):\n",
    "        from script_utils import setup_working_directory\n",
    "        setup_working_directory(working_folder)\n",
    "    else:\n",
    "        setup_working_directory_portable(working_folder)\n",
    "except (ImportError, NameError):\n",
    "    setup_working_directory_portable(working_folder)\n",
    "\n",
    "print(f\"üìÅ Training workspace: {working_folder}\")\n",
    "print(f\"ü§ñ Model ID: {model_id}\")\n",
    "print(f\"üìä Training parameters:\")\n",
    "print(f\"  - Max steps: {max_steps}\")\n",
    "print(f\"  - Epochs: {num_train_epochs}\")\n",
    "print(f\"  - Train batch size: {per_device_train_batch_size}\")\n",
    "print(f\"  - Eval batch size: {per_device_eval_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load pretrained feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "print(\"‚úì Feature extractor loaded\")\n",
    "\n",
    "# Check for required functions\n",
    "if create_dataset is None:\n",
    "    print(\"‚ùå create_dataset function not available\")\n",
    "    print(\"This is required for training. Please check module imports.\")\n",
    "    raise ImportError(\"Required MCAP utilities not available\")\n",
    "\n",
    "if spectrogram_from_timeseries is None:\n",
    "    print(\"‚ùå spectrogram_from_timeseries function not available\")\n",
    "    print(\"This is required for training. Please check module imports.\")\n",
    "    raise ImportError(\"Required MCAP utilities not available\")\n",
    "\n",
    "# Generate synthetic training data\n",
    "print(\"üî¨ Generating synthetic training datasets...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize the synthetic data generator\n",
    "try:\n",
    "    if SyntheticDataGenerator is not None:\n",
    "        generator = SyntheticDataGenerator()\n",
    "    else:\n",
    "        from synthetic import SyntheticDataGenerator\n",
    "        generator = SyntheticDataGenerator()\n",
    "    print(\"‚úì Synthetic data generator initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not initialize synthetic data generator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create base plan template\n",
    "base_plan = {\n",
    "    \"initialization\": {\n",
    "        \"pose\": {\n",
    "            \"origin\": {\"lat\": 38.446, \"lng\": -122.687, \"height\": 0.0},\n",
    "            \"position\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
    "            \"rotation\": {\"w\": 1.0, \"x\": 0.0, \"y\": 0.0, \"z\": 0.0}\n",
    "        },\n",
    "        \"start_time_ns\": 0,\n",
    "        \"sample_rate\": SAMPLE_RATE,\n",
    "        \"mag\": {\n",
    "            \"calibration\": {\n",
    "                \"bias\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
    "                \"matrix\": [[0.00053, 0.0, 0.0], [0.0, 0.00053, 0.0], [0.0, 0.0, 0.00053]]\n",
    "            }\n",
    "        },\n",
    "        \"acc\": {\n",
    "            \"calibration\": {\n",
    "                \"bias\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
    "                \"matrix\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]\n",
    "            }\n",
    "        },\n",
    "        \"gyro\": {\n",
    "            \"calibration\": {\n",
    "                \"bias\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
    "                \"matrix\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"segments\": []\n",
    "}\n",
    "\n",
    "# Define distortion levels and motion patterns\n",
    "distortion_configs = [\n",
    "    {\"level\": \"none\", \"value\": 0.0},\n",
    "    {\"level\": \"low\", \"value\": 1.0},\n",
    "    {\"level\": \"high\", \"value\": 2.5}\n",
    "]\n",
    "\n",
    "motion_patterns = [\n",
    "    {\"roll\": 0, \"pitch\": 0, \"yaw\": 0},      # Stationary\n",
    "    {\"roll\": 45, \"pitch\": 0, \"yaw\": 0},     # Roll motion\n",
    "    {\"roll\": 0, \"pitch\": 30, \"yaw\": 0},     # Pitch motion\n",
    "    {\"roll\": 0, \"pitch\": 0, \"yaw\": 60},     # Yaw motion\n",
    "    {\"roll\": 30, \"pitch\": 30, \"yaw\": 30},   # Complex motion\n",
    "]\n",
    "\n",
    "# Generate synthetic datasets\n",
    "file_pairs = []\n",
    "generated_files = []\n",
    "\n",
    "for seq_idx in range(NUM_SEQUENCES):\n",
    "    # Cycle through distortion levels and motion patterns\n",
    "    distortion = distortion_configs[seq_idx % len(distortion_configs)]\n",
    "    motion = motion_patterns[seq_idx % len(motion_patterns)]\n",
    "    \n",
    "    # Create plan for this sequence\n",
    "    plan = base_plan.copy()\n",
    "    plan[\"segments\"] = [\n",
    "        {\n",
    "            \"name\": f\"segment_{seq_idx}_{distortion['level']}\",\n",
    "            \"duration_s\": SEQUENCE_DURATION,\n",
    "            \"rotation_rpy_degrees\": motion,\n",
    "            \"magnetic_distortion\": distortion[\"value\"],\n",
    "            \"mag_distortion\": {\"level\": distortion[\"level\"]}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Generate filenames\n",
    "    sequence_name = f\"training_sequence_{seq_idx:02d}_{distortion['level']}\"\n",
    "    mcap_file = f\"{sequence_name}.mcap\"\n",
    "    labels_file = f\"{sequence_name}.labels.json\"\n",
    "    spectrogram_file = f\"{sequence_name}.spectrogram.mcap\"\n",
    "    \n",
    "    print(f\"üìä Generating sequence {seq_idx+1}/{NUM_SEQUENCES}: {sequence_name}\")\n",
    "    print(f\"  - Distortion: {distortion['level']} ({distortion['value']})\")\n",
    "    print(f\"  - Motion: roll={motion['roll']}¬∞, pitch={motion['pitch']}¬∞, yaw={motion['yaw']}¬∞\")\n",
    "    \n",
    "    try:\n",
    "        # Generate synthetic data\n",
    "        generator.generate(plan_data=plan, output_file=mcap_file, verbose=False)\n",
    "        \n",
    "        # Generate labels\n",
    "        generator.generate_labels(plan, labels_file)\n",
    "        \n",
    "        # Create spectrogram\n",
    "        print(f\"  üîÑ Creating spectrogram...\")\n",
    "        spectrogram_from_timeseries(mcap_file, spectrogram_file, feature_extractor=feature_extractor)\n",
    "        \n",
    "        # Verify files were created\n",
    "        if all(os.path.exists(f) for f in [mcap_file, labels_file, spectrogram_file]):\n",
    "            file_pairs.append([spectrogram_file, labels_file])\n",
    "            generated_files.extend([mcap_file, labels_file, spectrogram_file])\n",
    "            \n",
    "            # Show file sizes for verification\n",
    "            mcap_size = os.path.getsize(mcap_file)\n",
    "            spec_size = os.path.getsize(spectrogram_file)\n",
    "            print(f\"  ‚úì Generated: MCAP={mcap_size:,}B, Spectrogram={spec_size:,}B\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Failed to generate all files for {sequence_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error generating {sequence_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(file_pairs)} training sequences successfully!\")\n",
    "print(f\"üìÅ Total files created: {len(generated_files)}\")\n",
    "\n",
    "if len(file_pairs) == 0:\n",
    "    print(\"‚ùå No training data was generated!\")\n",
    "    print(\"üîß Check the synthetic data generator setup and try again\")\n",
    "    raise ValueError(\"No training data available\")\n",
    "\n",
    "# Show what was generated\n",
    "print(f\"\\nüìã Training Data Summary:\")\n",
    "distortion_counts = {}\n",
    "for spectrogram_file, labels_file in file_pairs:\n",
    "    # Read the labels to see what distortion levels we have\n",
    "    with open(labels_file, 'r') as f:\n",
    "        labels_data = json.load(f)\n",
    "        events = labels_data.get('events', [])\n",
    "        for event in events:\n",
    "            level = event.get('metadata', {}).get('mag_distortion', 'unknown')\n",
    "            distortion_counts[level] = distortion_counts.get(level, 0) + 1\n",
    "\n",
    "print(\"Distortion level distribution:\")\n",
    "for level, count in sorted(distortion_counts.items()):\n",
    "    print(f\"  - {level}: {count} sequences\")\n",
    "\n",
    "total_duration = len(file_pairs) * SEQUENCE_DURATION\n",
    "print(f\"Total training data: {total_duration} seconds ({total_duration/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "# we define which pretrained model we want to use and instantiate a feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace dataset from the generated synthetic data\n",
    "print(\"üîÑ Creating dataset from generated synthetic data...\")\n",
    "\n",
    "# Create dataset using the synthetic data file pairs\n",
    "dataset = create_dataset(\n",
    "    file_pairs=file_pairs,\n",
    "    use_unlabeled_sections=False,  # Use only labeled events for cleaner training\n",
    "    unlabeled_section_label=\"none\",  # Default for any unlabeled sections\n",
    "    aggregate_labels=True,\n",
    "    aggregate_label_dict={\n",
    "        \"0\": \"none\",    # Map numeric labels to descriptive names\n",
    "        \"1\": \"low\", \n",
    "        \"2\": \"high\",\n",
    "        \"none\": \"none\",  # Already correct\n",
    "        \"low\": \"low\",    # Already correct\n",
    "        \"high\": \"high\"   # Already correct\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"‚úì Dataset created from synthetic data\")\n",
    "\n",
    "# Save dataset\n",
    "dataset_dir = \"dataset\" if not DEBUG_MODE else \"dataset_debug\"\n",
    "dataset.save_to_disk(dataset_dir)\n",
    "print(f\"‚úì Dataset saved to: {dataset_dir}\")\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"\\nüìä Dataset Information:\")\n",
    "print(f\"  - Total samples: {len(dataset)}\")\n",
    "print(f\"  - Features: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Show label distribution\n",
    "if 'labels' in dataset.features:\n",
    "    label_feature = dataset.features['labels']\n",
    "    label_names = label_feature.names\n",
    "    print(f\"  - Classes ({len(label_names)}): {label_names}\")\n",
    "    \n",
    "    # Count samples per class\n",
    "    if len(dataset) > 0:\n",
    "        label_counts = {}\n",
    "        for sample in dataset:\n",
    "            label_name = label_names[sample['labels']]\n",
    "            label_counts[label_name] = label_counts.get(label_name, 0) + 1\n",
    "        \n",
    "        print(f\"  - Label distribution:\")\n",
    "        total_samples = len(dataset)\n",
    "        for label, count in sorted(label_counts.items()):\n",
    "            percentage = (count / total_samples) * 100\n",
    "            print(f\"    - {label}: {count} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Check for class balance\n",
    "        max_count = max(label_counts.values())\n",
    "        min_count = min(label_counts.values())\n",
    "        balance_ratio = min_count / max_count if max_count > 0 else 0\n",
    "        \n",
    "        if balance_ratio > 0.8:\n",
    "            print(f\"  ‚úÖ Well-balanced dataset (ratio: {balance_ratio:.2f})\")\n",
    "        elif balance_ratio > 0.5:\n",
    "            print(f\"  ‚úì Reasonably balanced dataset (ratio: {balance_ratio:.2f})\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Imbalanced dataset (ratio: {balance_ratio:.2f}) - consider generating more data\")\n",
    "\n",
    "# Show sample data structure\n",
    "if len(dataset) > 0:\n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nüìã Sample Data Structure:\")\n",
    "    for key, value in sample.items():\n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"  - {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  - {key}: {type(value)} = {value}\")\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(f\"\\nüêõ Debug mode: Using reduced dataset for fast testing\")\n",
    "    print(f\"   For production training, set DEBUG_MODE = False\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Production dataset ready for full training\")\n",
    "\n",
    "# Verify we have enough data for training\n",
    "min_samples_needed = 10\n",
    "if len(dataset) < min_samples_needed:\n",
    "    print(f\"‚ö†Ô∏è Warning: Only {len(dataset)} samples available\")\n",
    "    print(f\"   Recommended minimum: {min_samples_needed} samples\")\n",
    "    print(f\"   Consider increasing NUM_SEQUENCES or SEQUENCE_DURATION\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset size is sufficient for training ({len(dataset)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "print(\"ü§ñ Configuring model...\")\n",
    "\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Access the ClassLabel feature for the labels\n",
    "label_feature = dataset.features[\"labels\"]\n",
    "label_names = label_feature.names\n",
    "\n",
    "print(f\"‚úì Label names: {label_names}\")\n",
    "\n",
    "# Update model configuration for our specific task\n",
    "config.num_labels = len(label_names)\n",
    "config.label2id = {label: i for i, label in enumerate(label_names)}\n",
    "config.id2label = {i: label for label, i in config.label2id.items()}\n",
    "\n",
    "print(f\"‚úì Model configured for {config.num_labels} classes\")\n",
    "print(f\"  - Label mapping: {config.label2id}\")\n",
    "\n",
    "# Split training data if test split doesn't exist\n",
    "if \"test\" not in dataset:\n",
    "    print(\"üîÑ Splitting dataset into train/test...\")\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=42, stratify_by_column=\"labels\"\n",
    "    )\n",
    "    print(f\"‚úì Dataset split:\")\n",
    "    print(f\"  - Training samples: {len(dataset['train'])}\")\n",
    "    print(f\"  - Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Initialize model\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config)\n",
    "print(\"‚úì Model initialized with custom configuration\")\n",
    "\n",
    "# Training arguments - use the configuration from earlier\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./runs{'_debug' if DEBUG_MODE else ''}\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    logging_steps=eval_steps,\n",
    "    save_steps=eval_steps * 2,\n",
    "    max_steps=max_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=f\"./logs{'_debug' if DEBUG_MODE else ''}\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues in notebooks\n",
    ")\n",
    "\n",
    "print(\"‚úì Training arguments configured\")\n",
    "print(f\"  - Output directory: {training_args.output_dir}\")\n",
    "print(f\"  - Logging directory: {training_args.logging_dir}\")\n",
    "print(f\"  - Max steps: {training_args.max_steps}\")\n",
    "print(f\"  - Batch sizes: train={training_args.per_device_train_batch_size}, eval={training_args.per_device_eval_batch_size}\")\n",
    "\n",
    "# Load accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print(\"‚úì Metrics configured (accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"üèÉ Initializing trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")\n",
    "\n",
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"üìä Training on {len(dataset['train'])} samples, evaluating on {len(dataset['test'])} samples\")\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(\"üêõ Debug mode: Training will be quick but not optimal\")\n",
    "else:\n",
    "    print(\"‚è≥ Full training mode: This may take a while...\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"‚úì Training completed!\")\n",
    "print(f\"üìà Final training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"üìä Running final evaluation...\")\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"‚úì Evaluation completed!\")\n",
    "print(f\"üìä Final accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_dir = \"model\" if not DEBUG_MODE else \"model_debug\"\n",
    "print(f\"üíæ Saving model to {model_dir}...\")\n",
    "\n",
    "model.save_pretrained(model_dir)\n",
    "feature_extractor.save_pretrained(model_dir)\n",
    "\n",
    "print(f\"‚úì Model saved to {model_dir}/\")\n",
    "\n",
    "# Create model archive for upload\n",
    "import tarfile\n",
    "model_tar_filename = f\"{model_id}.model.tar.gz\"\n",
    "\n",
    "print(f\"üì¶ Creating model archive: {model_tar_filename}\")\n",
    "with tarfile.open(model_tar_filename, \"w:gz\") as tar:\n",
    "    tar.add(model_dir, arcname=\"model\")\n",
    "\n",
    "print(f\"‚úì Model archive created: {model_tar_filename}\")\n",
    "\n",
    "# Upload model archive\n",
    "print(\"üì§ Uploading model...\")\n",
    "try:\n",
    "    upload_with_prefix(nst_client, model_tar_filename, \"\", overwrite=True)\n",
    "    print(f\"‚úì Model uploaded successfully\")\n",
    "except NameError:\n",
    "    # Fallback upload\n",
    "    nst_client.upload(model_tar_filename, model_tar_filename, overwrite=True)\n",
    "    print(f\"‚ö†Ô∏è Model uploaded using legacy method\")\n",
    "\n",
    "print(\"üéâ Fine-tuning pipeline completed successfully!\")\n",
    "print(f\"\\nüìã Summary:\")\n",
    "print(f\"  - Model ID: {model_id}\")\n",
    "print(f\"  - Training samples: {len(dataset['train'])}\")\n",
    "print(f\"  - Test samples: {len(dataset['test'])}\")\n",
    "print(f\"  - Final accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "print(f\"  - Classes: {label_names}\")\n",
    "print(f\"  - Model saved: {model_dir}/\")\n",
    "print(f\"  - Archive: {model_tar_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
